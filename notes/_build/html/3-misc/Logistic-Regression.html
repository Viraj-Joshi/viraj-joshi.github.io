
<!DOCTYPE html>


<html lang="en" data-content_root="../" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Building up to Multinomial Logistic Regression &#8212; some technical perusings</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=884a7dd3" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '3-misc/Logistic-Regression';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="JiT (Just Image Transformers)" href="../2-diffusion-and-flow-models/JiT.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">some technical perusings</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Reinforcement Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../1-RL/Implicit-Value-Regularization.html">Implicit Value Regularization (IVR)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Diffusion and Flow Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../2-diffusion-and-flow-models/Score-Matching.html">Score Matching</a></li>




<li class="toctree-l1"><a class="reference internal" href="../2-diffusion-and-flow-models/JiT.html">JiT (Just Image Transformers)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Building up to Multinomial Logistic Regression</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/kvfrans/notes/blob/master/3-misc/Logistic-Regression.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Building up to Multinomial Logistic Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perceptron">Perceptron</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">Loss Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-of-negative-log-likelihood">Gradient of Negative Log Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logits">Logits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-cross-entropy-bce">Binary Cross Entropy (BCE)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connections-between-perceptron-and-logistic-regression">Connections between Perceptron and Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss">0/1 Loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perceptron-loss">Perceptron Loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-loss">Logistic Loss</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-of-logistic-regression-loss">Gradient of Logistic Regression loss</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiclass-perceptron">Multiclass Perceptron</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-logistic-regression-softmax-regression">Multinomial Logistic Regression / Softmax Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-function">Softmax Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding">One-hot Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy">Cross Entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#aside">Aside:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function-derivative">Loss Function Derivative</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-together-the-chain-rule">Putting together the chain rule</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#citations">Citations</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="building-up-to-multinomial-logistic-regression">
<h1>Building up to Multinomial Logistic Regression<a class="headerlink" href="#building-up-to-multinomial-logistic-regression" title="Link to this heading">#</a></h1>
<section id="perceptron">
<h2>Perceptron<a class="headerlink" href="#perceptron" title="Link to this heading">#</a></h2>
<p>The Perceptron is simply a linear classifier. It does classification using a linear combination of the features <span class="math notranslate nohighlight">\(f(\mathbf x)\)</span> with weights <span class="math notranslate nohighlight">\(\mathbf w\)</span>. Let this be the activation <span class="math notranslate nohighlight">\(h_\mathbf w(\mathbf x)\)</span>. The most basic perceptron is a binary classifier, which uses the sign of the activation as the class.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\text{classify}(\mathbf x) = 
\left\{
    \begin{array}{lr}
        +1, &amp; \text{if } h_\mathbf w(\mathbf x) = \mathbf w^T f(\mathbf x) &gt; 0\\
        -1, &amp; \text{if } h_\mathbf w(\mathbf x) = \mathbf w^T f(\mathbf x) &lt; 0
    \end{array}
\right\}
\end{equation}
\end{split}\]</div>
<p>If the data is linearly separable, perceptron is guaranteed to converge.</p>
<p>In order to find a decision boundary that does not go through the origin (which may be necessary for some problems), we will modify our feature and weights to add a bias term: add a feature to your sample feature vectors that is always 1, and add an extra weight for this feature to your weight vector. Doing so essentially
allows us to produce a decision boundary representable by <span class="math notranslate nohighlight">\(\mathbf w^T f(\mathbf x) + b  = 0\)</span> where <span class="math notranslate nohighlight">\(b\)</span> is the weighted bias
term (i.e. 1 * the last weight in the weight vector).</p>
<p><img alt="" src="../_images/decision_boundary.png" /></p>
<p>To find the best decision boundary parameterized by <span class="math notranslate nohighlight">\(\mathcal w\)</span>, given a dataset <span class="math notranslate nohighlight">\(\mathcal D\)</span>, we train by the following procedure:</p>
<p><img alt="" src="../_images/perceptron_algo.png" /></p>
<p>Note how the update rule on line 5 encapsulates three cases:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_{pred} = y_i\)</span> means do nothing to <span class="math notranslate nohighlight">\(\mathbf w\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y_{pred} = 1\)</span> but <span class="math notranslate nohighlight">\(y_i = -1\)</span>: In other words, the activation is too small. How we adjust w should strive to fix that and make the activation larger for that training sample. We can easily convince ourself that our update rule that adds <span class="math notranslate nohighlight">\(f(\mathbf x)\)</span> does increase the activation:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
h_{\mathbf w + f(\mathbf w)}(\mathbf x) = (\mathbf w + f(\mathbf x))^T f(\mathbf x) = \mathbf w^T f(\mathbf x) + f(\mathbf x)^T f(\mathbf x)
\]</div>
<p>The update rule adds <span class="math notranslate nohighlight">\(f(\mathbf x)^T f(\mathbf x)\)</span>, which is always positive. The activation of that sample is getting larger and more positive.</p>
<ol class="arabic simple" start="3">
<li><p><span class="math notranslate nohighlight">\(y_{pred} = -1\)</span> but <span class="math notranslate nohighlight">\(y_i = 1\)</span>: In other words, the activation is too big. Following the logic of 2), we see that our update rule must decrease the activation for that sample.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[h_{\mathbf w - f(\mathbf w)}(\mathbf x) = (\mathbf w - f(\mathbf x))^T f(\mathbf x) = \mathbf w^T f(\mathbf x) - f(\mathbf x)^T f(\mathbf x)
\]</div>
<p>which it does because it subtracts <span class="math notranslate nohighlight">\(f(\mathbf x)^T f(\mathbf x)\)</span>, which is always positive.</p>
<p>The summary of the update is that you add <span class="math notranslate nohighlight">\(f(\mathbf x)\)</span> when
<span class="math notranslate nohighlight">\(y\)</span> (the true label) is positive, and subtract it when <span class="math notranslate nohighlight">\(y\)</span> is negative.</p>
</section>
<section id="logistic-regression">
<h2>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Link to this heading">#</a></h2>
<p><img alt="" src="https://i.imgur.com/TToaVTP.png" /></p>
<p>Logistic Regression is also a binary classification model where because it is traditionally viewed in the lens of probability,the label space is <span class="math notranslate nohighlight">\(\mathcal Y = \{0,1\}\)</span>. The output variable <span class="math notranslate nohighlight">\(y_{i}\)</span> is a Bernoulli random variable (it can take only two values, either 1 or 0.</p>
<p>Unlike the perceptron outputting the class directly, we output the probability of the class a sample <span class="math notranslate nohighlight">\(\mathbf x\)</span> belongs to</p>
<div class="math notranslate nohighlight">
\[h_\mathbf w(\mathbf x) = \mathbf \sigma(\mathbf w^T f(\mathbf x))\]</div>
<p>using the logistic function <span class="math notranslate nohighlight">\(\sigma(z) = \frac{1}{1+e^{-z}}\)</span></p>
<p>The posterior is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
P(y|\mathbf x) = 
\left\{
    \begin{array}{lr}
        h_\mathbf w(\mathbf x), &amp; \text{if } y=1\\
        1-h_\mathbf w(\mathbf x), &amp; \text{if } y=0
    \end{array}
\right\}
\end{equation}
\end{split}\]</div>
<p>A more compact way of writing the piecewise function is</p>
<div class="math notranslate nohighlight">
\[P(y|\mathbf x) = h_\mathbf w(\mathbf x)^y\cdot(1-h_\mathbf w(\mathbf x))^{(1-y)}\]</div>
<p>If <span class="math notranslate nohighlight">\(P(y|\mathbf x) &gt; .5\)</span> (or equivalently <span class="math notranslate nohighlight">\(\mathbf w^T f(\mathbf x) &gt; 0\)</span>), we classify <span class="math notranslate nohighlight">\(\mathbf x\)</span> as 1 and 0 otherwise.</p>
<section id="loss-function">
<h3>Loss Function<a class="headerlink" href="#loss-function" title="Link to this heading">#</a></h3>
<p>Since a closed form solution for the optimal weights does not exist, we use MLE and maximize the likelihood of data.</p>
<p>That is
$<span class="math notranslate nohighlight">\(
\begin{equation}
\begin{split}
\max_w\prod\limits_{i}P(y_i|x_i)&amp;=\max_w\prod\limits_{i}h_\mathbf w(\mathbf x)^{y_i} \cdot (1-h_\mathbf w(\mathbf x))^{(1-y_i)} \\
&amp; \equiv \max_w\sum_{i} y_i \log h_\mathbf w(\mathbf x_i) + (1-y_i) \log (1- h_\mathbf w(\mathbf x_i))\\
\end{split}
\end{equation}
\)</span>$</p>
<p>It is easier to maximize the log due to floating point error introduced by maximizing the original.</p>
<p>Because most optimizer libraries use gradient descent, we take the negative of our objective to obtain an objective to minimize, negative log likelihood</p>
<div class="math notranslate nohighlight">
\[\min_\mathbf w-\sum_{i} y_i \log h_\mathbf w(\mathbf x_i) + (1-y_i) \log (1- h_\mathbf w(\mathbf x_i))\]</div>
</section>
<section id="gradient-of-negative-log-likelihood">
<h3>Gradient of Negative Log Likelihood<a class="headerlink" href="#gradient-of-negative-log-likelihood" title="Link to this heading">#</a></h3>
<p>Note $<span class="math notranslate nohighlight">\(\frac{d}{dz} \sigma(z) = \sigma(z)(1-\sigma(z))\)</span>$</p>
<p>The <span class="math notranslate nohighlight">\(j\)</span> th entry of the gradient is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\frac{\partial \mathcal L}{\partial \mathbf w_j} &amp;= \frac{\partial \mathcal L}{\partial \mathbf w_j}-\bigg[y\log h_\mathbf w(\mathbf x) + (1-y) \log (1-h_\mathbf w(\mathbf x))\bigg] \\
&amp; = -\bigg(\frac{\partial }{\partial \mathbf w_j}y\log h_\mathbf w(\mathbf x) + \frac{\partial }{\partial \mathbf w_j}(1-y) \log (1-h_\mathbf w(\mathbf x))\bigg) \\
&amp; = -\frac{y}{h_\mathbf w(\mathbf x)} \frac{\partial }{\partial \mathbf w_j} h_\mathbf w(\mathbf x) - \frac{(1-y)}{1-h_\mathbf w(\mathbf x)}\frac{\partial }{\partial \mathbf w_j} (1-h_\mathbf w(\mathbf x)) \\
&amp; = - \bigg(\frac{y}{h_\mathbf w(\mathbf x)} - \frac{(1-y)}{1-h_\mathbf w(\mathbf x)}\bigg) \frac{\partial }{\partial \mathbf w_j} h_\mathbf w(\mathbf x)\\
&amp; = -\bigg(\frac{y-h_\mathbf w(\mathbf x)}{h_\mathbf w(\mathbf x) (1-h_\mathbf w(\mathbf x))}\bigg)\frac{\partial }{\partial \mathbf w_j} h_\mathbf w(\mathbf x)\\
&amp; =  -\bigg(\frac{y-h_\mathbf w(\mathbf x)}{h_\mathbf w(\mathbf x) (1-h_\mathbf w(\mathbf x))}\bigg)h_\mathbf w(\mathbf x)(1-h_\mathbf w(\mathbf x))\mathbf w_j\\ 
&amp; = (h_\mathbf w(\mathbf x) - y) \mathbf x_j
\end{split}
\end{split}\]</div>
</section>
<section id="logits">
<h3>Logits<a class="headerlink" href="#logits" title="Link to this heading">#</a></h3>
<p>Generally, logits are simply the inputs to the last neurons layer.</p>
<p>In the context of logistic regression, this is simply <span class="math notranslate nohighlight">\(z=\mathbf w^T \mathbf x\)</span>.</p>
</section>
<section id="binary-cross-entropy-bce">
<h3>Binary Cross Entropy (BCE)<a class="headerlink" href="#binary-cross-entropy-bce" title="Link to this heading">#</a></h3>
<p><strong>BCE is exactly negative log likelihood</strong></p>
<div class="math notranslate nohighlight">
\[BCE(y,x|\theta) = -\sum_{i} y_i \log p_\theta(y|x_i) + (1-y_i) \log (1- p_\theta(y|x_i))\]</div>
<p>So maximizing the log likelihood(or minimizing the negative log likelihood) is equivalent to minimizing BCE.</p>
<p>In PyTorch, BCEWithLogitsLoss expects the logits as input.</p>
</section>
</section>
<section id="connections-between-perceptron-and-logistic-regression">
<h2>Connections between Perceptron and Logistic Regression<a class="headerlink" href="#connections-between-perceptron-and-logistic-regression" title="Link to this heading">#</a></h2>
<section id="loss">
<h3>0/1 Loss<a class="headerlink" href="#loss" title="Link to this heading">#</a></h3>
<p>One might think the ideal loss would be 0 if the instance is correctly classfied and 1 otherwise.
$<span class="math notranslate nohighlight">\(
\begin{equation}
\mathcal L_{0/1}(y,w^Tf(\mathbf x)) = 
\left\{
    \begin{array}{lr}
        0 &amp; \text{if } y=sign(w^T f(\mathbf x))\\
        1 &amp; \text{if } \text{otherwise}
    \end{array}
\right\}
\end{equation}
\)</span>$
This is because the sum of zero-one losses is proportional to the error rate of the classifier on the training data. Since a low error rate is often the ultimate goal of classification, this may seem
ideal. But this loss has two problems - this is non-convex so gradient-based optimization can seriously struggle and more seriously, the derivative is mostly 0 or undefined everywhere.</p>
</section>
<section id="perceptron-loss">
<h3>Perceptron Loss<a class="headerlink" href="#perceptron-loss" title="Link to this heading">#</a></h3>
<p>The perceptron optimizes a loss that is has better optimizatin properties.
We can express the loss for the perceptron of a pair <span class="math notranslate nohighlight">\((\mathbf x,y)\)</span> in terms of the quantity <span class="math notranslate nohighlight">\(y\mathbf w^T f(\mathbf w)\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\mathcal L_{perc} = 
\left\{
    \begin{array}{lr}
        0 &amp; \text{if } y\mathbf w^T f(\mathbf x) &gt; 0\\
        -y\mathbf w^T f(\mathbf x) &amp; \text{if } y\mathbf w^T f(\mathbf w) \leq 0
    \end{array}
\right\}
\end{equation}
\end{split}\]</div>
<p>Remember if the ground-truth <span class="math notranslate nohighlight">\(y\)</span> is +1, a correct prediction from our model occurs iff <span class="math notranslate nohighlight">\(\mathbf w^T f(\mathbf x)\)</span> is greater than 0. Similarly, if the ground-truth <span class="math notranslate nohighlight">\(y\)</span> is -1, a correct prediction from our model occurs iff <span class="math notranslate nohighlight">\(\mathbf w^T f(\mathbf x)\)</span> is less than 0. So we can see <span class="math notranslate nohighlight">\(y\mathbf w^T f(\mathbf x)\)</span> is only greater than 0 when the model predicts correctly and should contribute 0 to the loss.</p>
<p>Otherwise, the loss is exactly proportional to how wrong the classification is.</p>
<p>Differentiating the loss with respect to <span class="math notranslate nohighlight">\(\mathbf w\)</span> yields <span class="math notranslate nohighlight">\(−yf(\mathbf x)\)</span> when <span class="math notranslate nohighlight">\(y\mathbf w^T f(\mathbf x)\)</span> is negative; this is a constant with respect to a particular training example. However, recall that this value is a loss that we are attempting to minimize, so we want to use gradient descent which will involve subtracting the gradient from the weights. We therefore recover the standard update rule: add <span class="math notranslate nohighlight">\(f(\mathbf x)\)</span> when <span class="math notranslate nohighlight">\(y\)</span> (the true label) is positive, and subtract it when <span class="math notranslate nohighlight">\(y\)</span> is negative.
i.e</p>
<ul class="simple">
<li><p>when <span class="math notranslate nohighlight">\(y_{pred}=-1\)</span> and <span class="math notranslate nohighlight">\(y_i=1\)</span>, then the gradient descent update is $<span class="math notranslate nohighlight">\(\mathbf w \leftarrow \mathbf w - (-y_{i}f(\mathbf x)) = \mathbf w + f(\mathbf x)\)</span>$</p></li>
<li><p>when <span class="math notranslate nohighlight">\(y_{pred}=1\)</span> and <span class="math notranslate nohighlight">\(y_i=-1\)</span>, then the gradient descent update is $<span class="math notranslate nohighlight">\(\mathbf w \leftarrow \mathbf w - (-y_{i}f(\mathbf x)) = \mathbf w - f(\mathbf x)\)</span>$</p></li>
</ul>
<p>This is the so called hinge loss.</p>
</section>
<section id="logistic-loss">
<h3>Logistic Loss<a class="headerlink" href="#logistic-loss" title="Link to this heading">#</a></h3>
<p>To see the connection to perception loss, we have to redefine our notation to use the label space <span class="math notranslate nohighlight">\(\mathcal Y=\{-1,+1\}\)</span> to match the perceptron’s label space</p>
<p>This does not change the result but it does change the compact way of rewriting the posterior and as a result, the loss function.</p>
<div class="math notranslate nohighlight">
\[P(y|\mathbf x) = \sigma(y\mathbf w^T \mathbf x)\]</div>
<p>because <span class="math notranslate nohighlight">\(P(y=-1|\mathbf x)=\sigma(-\mathbf w^T f(\mathbf x))=\frac{1}{1+e^{\mathbf w^T f(x)}}\)</span> and <span class="math notranslate nohighlight">\(P(y=+1|\mathbf x)=\sigma(\mathbf w^T f(\mathbf x))=\frac{1}{1+e^{-\mathbf w^T f(x)}}\)</span></p>
<p>Equivalently, the sigmoid can be written as
$<span class="math notranslate nohighlight">\(
\begin{equation}
\mathcal P(y|x) = 
\left\{
    \begin{array}{lr}
        \frac{e^{\mathbf w^T f(\mathbf x)}}{1+e^{\mathbf w^T f(\mathbf x)}} &amp; \text{if } y=+1\\
        \frac{1}{1+e^{\mathbf w^T f(\mathbf x)}} &amp; \text{if } y=-1
    \end{array}
\right\}
\end{equation}
\)</span>$</p>
<p>Following the same steps to derive the MLE for <span class="math notranslate nohighlight">\(\mathbf w\)</span> as before:
$<span class="math notranslate nohighlight">\(
 \begin{split}
\max_{\mathbf w}\prod\limits_{i}P(y_i|x_i)&amp;\equiv\max_{\mathbf w}\log\bigg(\prod_i P(y_i|\mathbf x_i) \bigg) \\
&amp; =\max_{\mathbf w}\sum_{i} \log P(y_i|x_i)\\
&amp;= \max_{\mathbf w} \sum_{i} -\log (1+e^{-y_i\mathbf w^T f(\mathbf x_i)})\\
\end{split}
\)</span>$</p>
<!-- \begin{split}
\max_{\mathbf w}\prod\limits_{i}P(y_i|x_i)&\equiv\max_{\mathbf w}\log\bigg(\prod_i P(y_i|\mathbf x_i) \bigg) \\
& =\max_{\mathbf w}\sum_{i} \log P(y_i|x_i)\\
&= \max_{\mathbf w} \sum_{i} \frac{1}{2} (1+y_i) \mathbf w^T f(\mathbf x_i) - \log (1+e^{\mathbf w^T f(\mathbf x_i)})\\
\end{split} -->
<p>Because we minimize loss functions, we minimize the negative of this objective to obtain:
$<span class="math notranslate nohighlight">\(
\begin{split}
\mathcal L (\mathcal D,\mathbf w) 
&amp;= \min_{\mathbf w} \sum_{i} \log (1+e^{-y_i\mathbf w^T f(\mathbf x_i)}) \\
&amp;=\min_{\mathbf w} \sum_i - y_i\mathbf w^T f(\mathbf x_i) + \log \big(1+e^{y_i\mathbf w^T f(\mathbf x_i)}\big)
\end{split}
\)</span>$</p>
<section id="gradient-of-logistic-regression-loss">
<h4>Gradient of Logistic Regression loss<a class="headerlink" href="#gradient-of-logistic-regression-loss" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\nabla_{\mathbf w} \bigg[\sum_i \log (1+e^{-y_i\mathbf w^T f(\mathbf x)})\bigg] 
&amp; = \nabla_{\mathbf w} \bigg[\sum_i - y_i\mathbf w^T f(\mathbf x_i) + \log \big(1+e^{y_i\mathbf w^T f(\mathbf x_i)}\big)\bigg] \\
&amp; = \sum_i -y_i f(\mathbf x_i) + \frac{e^{y_i\mathbf w^T f(\mathbf x_i)}}{1+e^{y_i\mathbf w^T f(\mathbf x_i)}} y_if(\mathbf x_i)\\
&amp; = \sum_i f(\mathbf x_i)y_i (-1 + P(Y=+1|\mathbf x_i))\\
\end{split} 
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\nabla_{\mathbf w} \bigg[\sum_{i} \frac{1}{2} (1+y_i) \mathbf w^T f(\mathbf x_i) - \log (1+e^{\mathbf w^T f(\mathbf x_i)})\bigg] 
&amp; = \sum_i \frac{1}{2} (1+y_i) f(\mathbf x_i) - \frac{e^{\mathbf w^T f(\mathbf x_i)}}{1+e^{\mathbf w^T f(\mathbf x_i)}} f(\mathbf x_i)\\
&amp; = \sum_i \bigg(\frac{1}{2}(1+y_i) - P(Y=+1|\mathbf x)\bigg) f(\mathbf x_i)\\
\end{split}
\end{split}\]</div>
<p>Using gradient descent to minimize, the update rule is
$<span class="math notranslate nohighlight">\(\mathbf w \leftarrow \mathbf w + \alpha \sum_i \big( 1 - P(Y=+1|\mathbf x)\big) y_if(\mathbf x_i)\)</span>$</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(y=+1\)</span>, we add some fraction of <span class="math notranslate nohighlight">\(f(\mathbf x)\)</span> to <span class="math notranslate nohighlight">\(\mathbf w\)</span>, but less so as the quantity <span class="math notranslate nohighlight">\(P(y = +1|x)\)</span> gets bigger</p></li>
<li><p>If <span class="math notranslate nohighlight">\(y=-1\)</span>, we subtract some fraction of <span class="math notranslate nohighlight">\(f(\mathbf x)\)</span> from <span class="math notranslate nohighlight">\(\mathbf w\)</span> , but more so as the quantity <span class="math notranslate nohighlight">\(P(y = +1|x)\)</span> gets bigger.</p></li>
</ul>
<p>We can see that logistic regression’s update step is a soft version of the perceptron update where instead of a 1/-1 or 0 as the coefficient for <span class="math notranslate nohighlight">\(f(\mathbf x)\)</span>, we have a real-valued coefficient that depends on how “off” the probability is from the correct value</p>
<p><img alt="" src="https://hackmd.io/_uploads/HktKNjf63.png" /></p>
<p>They both asymptote to a line with slope −1 as <span class="math notranslate nohighlight">\(yw^Tf(x)\)</span> becomes negative and asymptote to <span class="math notranslate nohighlight">\(y=0\)</span> as it becomes positive. The logistic function is smoother and nonzero at the origin, meaning that it prefers examples to be classified correctly by a larger margin. Because it never becomes exactly 0, continuing to
train a logistic regression classifier will lead to larger and larger weights</p>
<p>Log loss (the logistic regression loss), perceptron loss, and hinge loss are all surrogate losses that approximate the 0-1 loss and
are easier to optimize.</p>
</section>
</section>
</section>
<section id="multiclass-perceptron">
<h2>Multiclass Perceptron<a class="headerlink" href="#multiclass-perceptron" title="Link to this heading">#</a></h2>
<p>We can extend the peceptron to multi classes easily. The differences mainly lie in how to setup the weights and update them.</p>
<p>One approach is to have a weight vector for each class, compute a score for each class by taking the dot product of the feature <span class="math notranslate nohighlight">\(f(\mathbf x)\)</span> with each of the weight vectors <span class="math notranslate nohighlight">\(\mathbf w_{y}\)</span>, and use the label associated with the weight vector that gave the highest score as the prediction.</p>
<div class="math notranslate nohighlight">
\[y_{\text{pred}} = \arg\max_y \mathbf w_{y}^T \mathbf f(x)\]</div>
<p>The update rule changes in the following way:</p>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(y_{\text{pred}}=y_i\)</span>, do nothing as before</p></li>
<li><p>when <span class="math notranslate nohighlight">\(y_{\text{pred}} \neq y_i\)</span>, then we add the feature vector to the weight vector for the true class
to <span class="math notranslate nohighlight">\(y_{i}\)</span> and subtract the feature vector from the weight vector corresponding to the predicted class <span class="math notranslate nohighlight">\(y_{\text{pred}}\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}\mathbf w_{y_{i}} \leftarrow \mathbf w_{y_{i}} + f(\mathbf x) \\\end{split}\\\mathbf w_{y_{\text{pred}}} \leftarrow \mathbf w_{y_{\text{pred}}} - f(\mathbf x)\end{aligned}\end{align} \]</div>
<p>Intuitively, this is “lowering” the weight that caused incorrect prediction and “increasing” the weight that would cause a correct prediction</p>
</section>
<section id="multinomial-logistic-regression-softmax-regression">
<h2>Multinomial Logistic Regression / Softmax Regression<a class="headerlink" href="#multinomial-logistic-regression-softmax-regression" title="Link to this heading">#</a></h2>
<p>Softmax regression is the generalization of logistic regression that handles multi-class classification i.e <span class="math notranslate nohighlight">\(y_i\)</span> can take on <span class="math notranslate nohighlight">\(h\)</span> number of classes</p>
<p>The model is a 1-layer Neural Network</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>without</p></th>
<th class="head text-center"><p>w/softmax</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><img alt="" src="../_images/wo_softmax.png" /></p></td>
<td class="text-center"><p><img alt="" src="../_images/w_softmax.png" /></p></td>
</tr>
</tbody>
</table>
</div>
<p>One can visualize the left handside as training <span class="math notranslate nohighlight">\(h\)</span> logistic regression models. Each logistic regression model has a weight vector <span class="math notranslate nohighlight">\(w \in \mathbb{R}^{m}\)</span>, so stacking them together row-wise,  we have a weight matrix <span class="math notranslate nohighlight">\(W \in \mathbb{R}^{h \times m}\)</span></p>
<p>Each of the activations <span class="math notranslate nohighlight">\(a_1,..,a_h\)</span> produced by the lefthand side model do not sum to 1. Because we want an activation <span class="math notranslate nohighlight">\(a_i\)</span> to be the probability that an example belongs to each class, we apply the softmax function to the logits of the network to make the activations sum to 1.</p>
<p>This is the shown by the right-hand side, where the sigmoid at each of the last layers neuron is removed, and replaced by a softmax layer as the final layer. The prediction of <span class="math notranslate nohighlight">\(x_i\)</span> is the <span class="math notranslate nohighlight">\(\arg\max_i a_i\)</span></p>
<hr class="docutils" />
<section id="softmax-function">
<h3>Softmax Function<a class="headerlink" href="#softmax-function" title="Link to this heading">#</a></h3>
<p>The softmax activation function of a logit <span class="math notranslate nohighlight">\(z_t^i\)</span>  where <span class="math notranslate nohighlight">\(t\in \{1,\dots,h\}\)</span> is the class label and <span class="math notranslate nohighlight">\(i\)</span> indexes the training example, computes the probability or activation <span class="math notranslate nohighlight">\(P(y=t|z_t^i)=a^i_t =\text{softmax}(z_t^i)\)</span></p>
<div class="math notranslate nohighlight">
\[a_t^i=\text{softmax}(z_t^i) = \frac{e^{z_t^i}}{\sum_{j=1}^he^{z_j^i}}\]</div>
<p>In order to use vector notation,we stack the <span class="math notranslate nohighlight">\(h\)</span> activations for a the <span class="math notranslate nohighlight">\(i\)</span>th example into a row <span class="math notranslate nohighlight">\([a_1^i \cdots a_h^i]\)</span></p>
</section>
<section id="one-hot-encoding">
<h3>One-hot Encoding<a class="headerlink" href="#one-hot-encoding" title="Link to this heading">#</a></h3>
<p>Again, in order to use vector notation, we one-hot encode the class label.</p>
<p>E.g If we have the following training labels <span class="math notranslate nohighlight">\(y\)</span> with 4 classes, the one hot encoding is
$<span class="math notranslate nohighlight">\(
\begin{equation*}
\begin{pmatrix}
0 \\
1 \\
3 \\
2 \\
0 \\
\end{pmatrix}
\rightarrow
\begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 0 \\
\end{pmatrix}
\end{equation*}
\)</span>$</p>
<p>where each row of the one hot encoding matrix represents the <span class="math notranslate nohighlight">\(i\)</span>th label of <span class="math notranslate nohighlight">\(y\)</span></p>
</section>
<section id="cross-entropy">
<h3>Cross Entropy<a class="headerlink" href="#cross-entropy" title="Link to this heading">#</a></h3>
<p>This is the generalization of BCE to the multi-class case. As before we would like to minimize the negative log-likelihood.</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathbf w\text{|} \mathcal D) = \sum_{i=1}^n \sum_{j=1}^h -y_j^i \log(a_j^i)\]</div>
<p>assuming one-hot encoded labels and each activation has been passed through softmax.
Each multiplication in the inner loop is element wise.</p>
<p>If h=2 (binary case), the CE term is BCE</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\begin{split}
CE &amp;= \sum_{i=1}^n -y_1^i \log a_1^i-y_2^i\log a_2^i  \\
&amp; = \sum_{i=1}^n -y_1^i \log a_1^i- y_2^i\log (1-a_1^i) \quad \text{by $a_1^i + a_2^i$ = 1}\\
&amp; = -\sum_{i=1}^n y_1^i \log a_1^i+ (1-y_1^i)\log (1-a_1^i) \quad \text{by $y_1^i$ =1 implies $y_2^i$ = 0}\\
\end{split}
\end{equation}
\end{split}\]</div>
<!-- Ex: The softmax output is the rowwise softmax of the feature matrix multiplied by the weights $XW$ -->
<p>Ex: For <span class="math notranslate nohighlight">\(h=3\)</span>, here is  a visual on  calculating CE</p>
<p><img alt="" src="https://i.imgur.com/KKcjz4N.png" /></p>
<p>See how each <span class="math notranslate nohighlight">\(\mathcal{L^i}\)</span> is the cross entropy of the <span class="math notranslate nohighlight">\(i\)</span> th training example</p>
<p>See how Negative Log Likelihood is CE</p>
<section id="aside">
<h4>Aside:<a class="headerlink" href="#aside" title="Link to this heading">#</a></h4>
<p>In pytorch, softmax(dim=d) computes the softmax along the dimension d s.t every slice along dim will sum to 1.
In PyTorch, cross entropy takes logits as input and returns the mean over the CE of each example.</p>
</section>
</section>
<section id="loss-function-derivative">
<h3>Loss Function Derivative<a class="headerlink" href="#loss-function-derivative" title="Link to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{N \times M}\)</span>, <span class="math notranslate nohighlight">\(Y \in \mathbb{R}^{N \times h}\)</span> be one-hot labels, and <span class="math notranslate nohighlight">\(W \in \mathbb{R}^{M \times h}\)</span></p>
<p>Our forward pass is <span class="math notranslate nohighlight">\(A = softmax(\underbrace{XW}_{Z})\)</span></p>
<p>We can rewrite the loss in matrix form</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} (W)= -\text{mean} (\sum_{j=1}^h (Y*\log A)_j) = -\text{mean} (\sum_{j=1}^h(Y*\log \text{softmax}(\underbrace{XW,dim=1}_{Z}))_j)\]</div>
<p>We want <span class="math notranslate nohighlight">\(\frac{\partial\mathcal{L}}{\partial W}\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial\mathcal{L}}{\partial b}\)</span> for gradient descent.</p>
<p>Assume we have one example to make notation cleaner.</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} (W)= -\sum_{j=1}^h (y*\log \underbrace{\text{softmax}(\underbrace{xW}_{z}}_{a}))_j \]</div>
<p>The loss simply sums the row vector because <span class="math notranslate nohighlight">\(y,x,z\)</span> and <span class="math notranslate nohighlight">\(a\)</span> are all row vectors</p>
<p>We can build up to the full gradient <span class="math notranslate nohighlight">\(\frac{\partial\mathcal{L}}{\partial W}\)</span>, by examining its entries <span class="math notranslate nohighlight">\(\frac{\partial\mathcal{L}}{\partial W_{ij}}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\frac{\partial \mathcal{L}}{\partial W_{ij}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i} \cdot \frac{\partial z_i}{\partial W_{ij}}
\end{aligned}\]</div>
<p>Lets examine the first term ,<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial z_i}\)</span>,the derivative of a scalar w.r.t an entry of a logits vector <span class="math notranslate nohighlight">\(z\)</span>. It is a bit tricky because we have to consider that perturbing <span class="math notranslate nohighlight">\(z_i\)</span> will effect ALL other probabilties due to the denominator in softmax, so we sum the effect</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathcal{L}}{\partial z_i} = \sum_{j=1}^h \frac{\partial \mathcal{L}}{\partial a_j} \frac{\partial a_j}{\partial z_i}\]</div>
<p>We can also bundle the bias into the weight vector as a column to avoid deriving an expression for <span class="math notranslate nohighlight">\(\frac{\partial\mathcal{L}}{\partial b}\)</span></p>
<hr class="docutils" />
<p>The first part of our expression, <span class="math notranslate nohighlight">\(\frac{\partial\mathcal{L}}{\partial a}\)</span>, is taking a derivative of a scalar w.r.t a vector <span class="math notranslate nohighlight">\(a\)</span>.</p>
<p>Each entry of that gradient is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\begin{split}
\frac{\partial\mathcal{L}}{\partial a_i} &amp; = \frac{\partial}{\partial a_i}
\left[\sum_{j=1}^h -y_j \log a_j\right]\\
&amp; = \frac{\partial}{\partial a_i}[-y_i \log a_i] \\
&amp; = -y_i/a_i
\end{split}
\end{equation}
\end{split}\]</div>
<hr class="docutils" />
<p>The second part of our expression, <span class="math notranslate nohighlight">\(\frac{\partial a}{\partial z}\)</span>, is taking the derivative of a vector w.r.t another vector. This is a matrix.
When one takes the derivative of <span class="math notranslate nohighlight">\(a_i\)</span> with its corresponding entry <span class="math notranslate nohighlight">\(z_i\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\begin{split}
\frac{\partial\mathcal{a_i}}{\partial z_i} &amp; = \frac{\partial}{\partial z_i}
\left[\frac{e^{z_i}}{\sum_{j=1}^h e^{z_j}}\right]\\
&amp; = 
\left[\frac{(\sum_{j=1}^h e^{z_j}) \frac{\partial}{\partial z_i}e^{z_i}-e^{z_i} \frac{\partial}{\partial z_i}\sum_{j=1}^h e^{z_j}}{(\sum_{j=1}^h e^{z_j})^2}\right]\\
&amp; = 
\left[\frac{(\sum_{j=1}^h e^{z_j}) e^{z_i} - e^{z_i}e^{z_i}}{(\sum_{j=1}^h e^{z_j})^2}\right] \\
&amp; = \frac{e^{z_i}(\left[\sum_{j=1}^h e^{z_j}\right]-e^{z_i})}{(\sum_{j=1}^h e^{z_j})^2} \\
&amp; = \frac{e^{z_i}}{\sum_{j=1}^h e^{z_j}}\cdot
\frac{\left[\sum_{j=1}^h e^{z_j}\right]-e^{z_i}}{\sum_{j=1}^h e^{z_j}}\\
&amp; = a_i (1-a_i)\\
\end{split}
\end{equation}
\end{split}\]</div>
<p>Similarly, for <span class="math notranslate nohighlight">\(i \neq k\)</span> :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\begin{split}
\frac{\partial\mathcal{a_i}}{\partial z_k} &amp; = \frac{\partial}{\partial z_k}
\left[\frac{e^{z_i}}{\sum_{j=1}^h e^{z_j}}\right]\\
&amp; = 
\left[\frac{(\sum_{j=1}^h e^{z_j}) \frac{\partial}{\partial z_k}e^{z_i}-e^{z_i} \frac{\partial}{\partial z_k}\sum_{j=1}^h e^{z_j}}{(\sum_{j=1}^h e^{z_j})^2}\right]\\
&amp; = 
\left[\frac{(\sum_{j=1}^h e^{z_j}) 0 - e^{z_i}e^{z_k}}{(\sum_{j=1}^h e^{z_j})^2}\right] \\
&amp; = \frac{0-e^{z_i}e^{z_k}}{(\sum_{j=1}^h e^{z_j})^2} \\
&amp; = \frac{-e^{z_i}}{\sum_{j=1}^h e^{z_j}}\cdot
\frac{e^{z_k}}{\sum_{j=1}^h e^{z_j}}\\
&amp; = -a_i a_k\\
\end{split}
\end{equation}
\end{split}\]</div>
<p>Using the two derivations, we can write the Jacobian matrix <span class="math notranslate nohighlight">\(\mathbf Jac(A) \in \mathbb{R}^{h\times h}\)</span> of activations.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation*}
Jac(A)_{n,n} = 
\begin{pmatrix}
\frac{\partial a_1}{\partial z_1} &amp; \frac{\partial a_1}{\partial z_2} &amp; \cdots &amp; \frac{\partial a_1}{\partial z_h} \\
\frac{\partial a_2}{\partial z_1} &amp; \frac{\partial a_2}{\partial z_2} &amp; \cdots &amp; \frac{\partial a_2}{\partial z_h} \\
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
\frac{\partial a_h}{\partial z_1} &amp; \frac{\partial a_n}{\partial z_2} &amp; \cdots &amp; \frac{\partial a_h}{\partial z_h} 
\end{pmatrix}
\end{equation*}
\end{split}\]</div>
<p>This can be more efficiently written, by first observing</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\frac{\partial a_i}{\partial z_k} = 
\begin{cases} 
      a_i (1-a_k) &amp; i = k \\
      -a_i a_k &amp; i \neq k \\
   \end{cases}
  = (a_i)(\delta_{i,k}-a_k)
\end{equation}
\end{split}\]</div>
<p>Which in matrix form is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\begin{split}
\frac{\partial \mathbf a}{\partial \mathbf z} &amp; = \mathbf a1_k^T \circ(\mathbf I - 1_k \mathbf a^T) \\
&amp; =\begin{pmatrix}
a_{1} \\
\vdots \\
a_{h}\\
\end{pmatrix}
\begin{pmatrix}
1 \cdots 1
\end{pmatrix}
\circ
\left[\begin{pmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 1 &amp; \cdots &amp; 0 \\
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
0 &amp; 0 &amp; \cdots &amp; 1 
\end{pmatrix}-
\begin{pmatrix}
1 \\
\vdots \\
1\\
\end{pmatrix}
\begin{pmatrix}
a_1 \cdots a_h
\end{pmatrix}\right] \\
&amp; = \begin{bmatrix} 
        a_1 &amp; \cdots &amp; a_1 \\ 
        a_2 &amp; \cdots &amp; a_2 \\ 
        \vdots &amp; \ddots &amp; \vdots 
        \\ a_h &amp; \cdots &amp; a_h 
    \end{bmatrix} \circ
\left (\mathbf I -
            \begin{bmatrix} 
                a_1 &amp; \cdots &amp; a_1 \\ 
                a_2 &amp; \cdots &amp; a_2 \\ 
                \vdots &amp; \ddots &amp; \vdots \\ 
                a_h &amp; \cdots &amp; a_h 
            \end{bmatrix}^\top
\right ) \\
&amp; = \begin{bmatrix} 
        a_1 (1-a_1) &amp; a_1 (-a_2)  &amp; \cdots &amp; a_1 (-a_h) \\
        a_2 (-a_1)  &amp; a_2 (1-a_2) &amp; a_2 (-a_3) &amp; \vdots \\
        a_3 (-a_1)  &amp; a_3 (-a_2) &amp; \ddots &amp; \vdots\\
        a_h (-a_1) &amp; a_h(-a_2)&amp; \cdots &amp;a_h (1-a_h)
    \end{bmatrix}\\
&amp; = \mathbf Jac(A)
\end{split}
\end{equation}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(1_k\)</span> is the ones column vector of (h by 1), <span class="math notranslate nohighlight">\(\circ\)</span> is the element wise product, and <span class="math notranslate nohighlight">\(\mathbf I\)</span> is <span class="math notranslate nohighlight">\(h\times h\)</span> identity</p>
<hr class="docutils" />
<p>Lastly, the chain rule <span class="math notranslate nohighlight">\(\frac{\partial z_i}{\partial W_{ij}}\)</span> is the derivative of a logit with respect to a weight entry in <span class="math notranslate nohighlight">\(W\)</span></p>
<p>Recall <span class="math notranslate nohighlight">\(z_i\)</span> is the ith entry of <span class="math notranslate nohighlight">\(x\cdot W\)</span></p>
<div class="math notranslate nohighlight">
\[\frac{\partial z_{i}}{\partial W_{ij}} = \frac{\partial }{\partial W_{ij}} \left[x_1 * W_{i1} + \cdots + x_M * W_{iM}\right] = x_i\]</div>
</section>
<section id="putting-together-the-chain-rule">
<h3>Putting together the chain rule<a class="headerlink" href="#putting-together-the-chain-rule" title="Link to this heading">#</a></h3>
<p>We rewrite the Chain Rule summation using the first term’s gradient derivation</p>
<div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial z_i} = \sum_{h} \left( - \frac{y_h}{p_h} \right) \frac{\partial a_h}{\partial z_i}\]</div>
<p>Substitute the jacobian formula into the second term and split this sum into the case where <span class="math notranslate nohighlight">\(h=i\)</span> and <span class="math notranslate nohighlight">\(h \neq i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial z_i} = \underbrace{\left(-\frac{y_i}{a_i}\right) a_i(1-a_i)}_{k=i} + \sum_{k \neq i} \underbrace{\left(-\frac{y_k}{a_k}\right) (-a_k a_i)}_{k \neq i}\]</div>
<p>Simplify the fractions (the <span class="math notranslate nohighlight">\(a\)</span>’s cancel out):</p>
<div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial z_i} = -y_i(1-a_i) + \sum_{k \neq i} y_k a_i\]</div>
<p>Expand the first term:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial z_i} = -y_i + y_i a_i + \sum_{k \neq i} y_k a_i\]</div>
<p>Factor out <span class="math notranslate nohighlight">\(p_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial z_i} = -y_i + a_i \underbrace{\left( y_i + \sum_{k \neq i} y_k \right)}_{\text{Sum of all } y}\]</div>
<p>Since <span class="math notranslate nohighlight">\(Y\)</span> is one-hot encoded, the sum of all <span class="math notranslate nohighlight">\(y_k\)</span> is exactly 1.</p>
<div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial z_i} = \delta_i:= a_i - y_i\]</div>
<p>Putting it together,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial \mathcal{L}}{\partial W_{ij}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i} \cdot \frac{\partial z_i}{\partial W_{ij}} \\
&amp;= (a_i - y_i) \cdot x_i
\end{aligned}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\implies \frac{\partial \mathcal{L}}{\partial W} = 
\begin{pmatrix}
x_1 \delta_1 &amp; x_1 \delta_2 &amp; \dots &amp; x_1 \delta_h \\
x_2 \delta_1 &amp; x_2 \delta_2 &amp; \dots &amp; x_2 \delta_h \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_M \delta_1 &amp; x_M \delta_2 &amp; \dots &amp; x_M \delta_h
\end{pmatrix}\end{split}\]</div>
<p>So, for a single example (<span class="math notranslate nohighlight">\(N=1\)</span>), the gradient  with respect to the weights <span class="math notranslate nohighlight">\(W\)</span>  is:$<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial W} = \mathbf{x}^T (\mathbf{a} - \mathbf{y})\)</span>$</p>
<p>And for <span class="math notranslate nohighlight">\(N\)</span> examples,</p>
<div class="math notranslate nohighlight">
\[\nabla_W L = X^T (A-Y)\]</div>
<p>This is identical to the gradient update for the two class case, where now every term is a matrix and specfically, A is a matrix of probabilties <span class="math notranslate nohighlight">\((N,h)\)</span>, instead of a vector <span class="math notranslate nohighlight">\((N,)\)</span>.</p>
<hr class="docutils" />
<p>Assume we have a model with 3 classes and each example has 2 features.
<img alt="" src="https://i.imgur.com/654dRYV.gif" /></p>
<p>Then, our chain rule is summing contributions e.g</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\begin{split}
\frac{dL}{dw_{1,1}} &amp; = \sum_{i=1}^3 \left (\frac{dL}{da_i} \right) \left (\frac{da_i}{dz_1} \right) \left(\frac{dz_1}{dw_{1,1}} \right ) \\
&amp; = -(y_1/a_1)(a_1(1-a_1))x_1 + -(y_2/a_2)(-a_2a_1)x_1 + -(y_3/a_3)(-a_3a_1)x_1
\end{split}
\end{equation}
\end{split}\]</div>
</section>
</section>
<section id="citations">
<h2>Citations<a class="headerlink" href="#citations" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/perc-lr-connections.pdf">https://www.cs.utexas.edu/~gdurrett/courses/online-course/perc-lr-connections.pdf</a></p>
<p>Sebastian Raschka: <a class="reference external" href="https://www.youtube.com/watch?v=10PTpRRpRk0">https://www.youtube.com/watch?v=10PTpRRpRk0</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./3-misc"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../2-diffusion-and-flow-models/JiT.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">JiT (Just Image Transformers)</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perceptron">Perceptron</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">Loss Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-of-negative-log-likelihood">Gradient of Negative Log Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logits">Logits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-cross-entropy-bce">Binary Cross Entropy (BCE)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connections-between-perceptron-and-logistic-regression">Connections between Perceptron and Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss">0/1 Loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perceptron-loss">Perceptron Loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-loss">Logistic Loss</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-of-logistic-regression-loss">Gradient of Logistic Regression loss</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiclass-perceptron">Multiclass Perceptron</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-logistic-regression-softmax-regression">Multinomial Logistic Regression / Softmax Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-function">Softmax Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding">One-hot Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy">Cross Entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#aside">Aside:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function-derivative">Loss Function Derivative</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-together-the-chain-rule">Putting together the chain rule</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#citations">Citations</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Viraj Joshi
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>